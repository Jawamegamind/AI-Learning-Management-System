--- New Retrieval ---
Optimized Query: Generate a quiz on tokenization and embeddings, testing their use and importance for pertaining of LLMs.
Context:
Lecture 4: Tokenization, Embedding and Pre-Training of LLM
Syed Babar Ali School of  Science and Engineering LUMS
Spring 2025
Problem Solving ‚Äì Artificial Intelligence ‚Äì Uncertainty
AI-602: Large Language Models 
Systems
BPE: Pitfall
Now that we have tokenized text, let's 
explore how we represent these 
tokens using embeddings.
Embedding ‚Äì Next Step after Tokenization
‚ÄúIam a student‚Äù
 Tokenizer Token 
Embedding
I
am
a
student
1
2
3
4
D-dimensional 
Vectors
Tokenization ‚Äì Why and What?
What is Tokenization?
Tokenization is the process of breaking down text into 
smaller units called tokens, which can be words, subwords, 
characters, or symbols. It is a fundamental step in NLP and 
LLMs, serving as the foundation for embedding and model 
training.
Why is Tokenization Important?
‚Ä¢Converts raw text into manageable units for machine 
learning models.
‚Ä¢Helps computers process and understand human language 
efficiently.
‚Ä¢Handles unknown words, multiple languages, and large 
datasets effectively.
Embeddings: Starting from Basics
‚ÄúIam a student‚Äù
Tokenizer 
VocabularySize:
ùëá
T okenEmbedding 
EmbeddingDim: 
D
D[0,
182,
874,
19084]
The roleof theembeddingis to 
assigna vectorto eachtoken
D = 2
0,‚ÄúI"
128,‚Äúam"
874,‚Äúa"
19084,‚Äústudent"
ùë•ùúñ‚ÑùùëÅ√óD
Applications - LLM Tokenizer
‚Ä¢ Used in basic NLP tasks like sentiment analysis, search 
engines, and rule-based text processing.
Word-Level Tokenization 
(Space-based, Rule-based, 
Punctuation-based)
‚Ä¢ Essential for LLMs & deep learning models  (BERT, GPT, T5).
‚Ä¢ Used in machine translation, speech recognition, and OCR
for multilingual text.
Subword Tokenization 
(WordPiece, BPE, 
SentencePiece, Unigram)
‚Ä¢ Applied in spell checking, noisy text processing, 
handwriting recognition, and code tokenization .
‚Ä¢ Useful for low-resource languages and text generation .
Character-Level 
Tokenization
Agenda
‚Ä¢ Why Train an LLM from scratch
‚Ä¢ T okenization
‚Ä¢ Vector Embeddings
‚Ä¢ Transformers
‚Ä¢ Scaling Law
‚Ä¢ Hardware Considerations
‚Ä¢ Pre-Training
Agenda
‚Ä¢ Why Train an LLM from scratch
‚Ä¢ T okenization
‚Ä¢ Vector Embeddings
‚Ä¢ Transformers
‚Ä¢ Scaling Law
‚Ä¢ Hardware Considerations
‚Ä¢ Pre-Training
Agenda
‚Ä¢ Why Train an LLM from scratch
‚Ä¢ T okenization
‚Ä¢ Vector Embeddings
‚Ä¢ Transformers
‚Ä¢ Scaling Law
‚Ä¢ Hardware Considerations
‚Ä¢ Pre-Training
Agenda
‚Ä¢ Why Train an LLM from scratch
‚Ä¢ T okenization
‚Ä¢ Vector Embeddings
‚Ä¢ Transformers
‚Ä¢ Scaling Law
‚Ä¢ Hardware Considerations
‚Ä¢ Pre-Training
--- End Retrieval ---
--- New Retrieval ---
Optimized Query: Generate a quiz on the different frameworks of converting an business problem to its AI solution.
Context:
Problem Formulation Frameworks
‚Ä¢ You have a brilliant idea for a new AI 
project. 
o To make it happen, you need to 
convince management to fund your 
idea.
‚Ä¢ AI projects are hard to define
‚Ä¢ Several frameworks to guide problem 
formulation
o AI Canvas
o CRISP-DM
o Design Thinking
Design Thinking: A more human centric approach in AI
‚Ä¢ Empathize
o Deep understanding of the problems users face
o Context they face
o Involves conducting user interviews, surveys, and observational studies 
to uncover insights
‚Ä¢ Define
o Clearly articulate the problem
o For example, in developing a healthcare AI, defining the problem might 
involve specifying how the technology can assist healthcare 
professionals in diagnosis or treatment planning
‚Ä¢ Ideate
o Encourages creative thinking and brainstorming to generate a wide range 
of potential solutions
o Explore different algorithms for solutions
‚Ä¢ Prototype
o Creating tangible representations of the selected ideas
o Allows developers to gather valuable feedback from users and 
stakeholders, iterating on the design based on real-world insightss
‚Ä¢ Test
o Iterative
o Validates the solution with real -world users
Understanding, Data Understanding, 
Data Preparation, Modeling, 
Evaluation, Deployment
Key areas: Problem Statement, AI 
Objectives, AI Techniques, Data, 
Success Metrics, Challenges, 
Stakeholders
Flexibility Flexible, can iterate between stages 
based on findings
Structured framework to quickly 
visualize key elements of an AI project
Best Use Case Projects involving data analysis and 
model building
AI-driven projects focused on aligning 
technology with business goals
Methodology TypeProcess-driven, cyclical Strategic, visual, and ideation-focused
End Goal Building predictive models or insights 
from data
Defining and aligning AI solutions with 
business objectives
Primary Users Data scientists, analysts, project 
managers
AI project teams, strategists, and 
business stakeholders
Use case evaluation
‚Ä¢ Ask why to build application
o "Solving the wrong problem is like climbing up a ladder that‚Äôs 
leaning against the wrong wall."
‚Ä¢ If you don't!
o Competitors with AI can make you obsolete
o Miss opportunities to boost profits and productivity.
‚Ä¢ Considerations:
o Existential threat                   Build in -house
o Outsource                   AI to boost profits and productivity
Lecture 3: Business Problem & Dataset Engineering for LLMs
LUMS Syed Babar Ali School of Science and Engineering 
Spring 2025
Problem Solving ‚Äì Artificial Intelligence ‚Äì Uncertainty
AI-602: Large Language Models 
Systems
Example
Approach Example Scenario Why It's Ideal
AI Canvas AI-driven pricing optimization 
system
Focus on business alignment, 
strategy, and maximizing 
profits.
CRISP-DM
Predicting patient survival 
rates using historical medical 
data
Structured approach to data 
understanding, model 
building, and evaluation.
Design Thinking AI-powered virtual assistant 
for elderly users
Emphasis on empathy, user 
experience, and iterative, 
user-centered design.
AI Canvas: Ingredients
‚Ä¢ Data
o How much data do you need?
o Do you have already a prepared dataset or 
do you need to source it? 
o Does it have to be labeled? 
o What data format are you expecting?
‚Ä¢ Skills
o Computer vision or natural language 
understanding? 
o Software developer roles?
o Product Manager and a UX Designer to 
gather customer requirements and to design 
a workflow?
‚Ä¢ Output Block
o Quantify Business success
o Quantitative Evaluation metric
TOPICS
1. Why is AI Engineering different?
2. Problem Framing in AI
3. The Role of Data in AI Applications
4. Techniques for Dataset Engineering
‚ñ™ Traditional Techniques
‚ñ™ AI based Techniques
Problem Solving ‚Äì Artificial Intelligence ‚Äì Uncertainty
AI-602: Large Language Models 
Systems
AI Canvas
‚Ä¢ AI Canvas is a strategic framework 
for structuring AI initiatives
‚Ä¢ Helps teams align on the key 
elements necessary for a successful 
AI-driven solution. 
o Helps to align stakeholders
‚Ä¢ The AI Project Canvas consists of 
four distinct parts:
o Value Proposition (center)
o Ingredients (Left)
o Integration for Customers (Right)
o Financing on the bottom (Bottom)
AI Canvas: Integration for Customers
‚Ä¢ Integration
o Integration into existing Software Architecture
o Where does it fit into the backend? 
o How will the customer engage with your model? 
o Will you use a microservice, monolith, or predict on-
the-fly during streaming?
‚Ä¢ Stakeholders
o Internal 
o External
‚Ä¢ Customers
o Who is the Customer that you are designing the 
project for? 
o Detail about your different customer groups to guide 
your decision-making throughout the process.
--- End Retrieval ---
--- New Retrieval ---
Optimized Query: Make a quiz testing the broad techniques to optimize training and inference, specifically MoE, PEFT and Quantization.
Context:
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matters?  
‚Ä¢ Training Efficiency
‚Ä¢ PEFT  
‚Ä¢ MoE   
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation   
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inferance Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inferance Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inferance Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Conclusion
Technique Pre-training Fine-tuning 
(Post-training) LLM Deployment Inference Purpose
PEFT (LoRA, 
Adapters, Prefix-
Tuning)
 No
  Yes
  Yes
  Yes
Efficient fine-
tuning by training 
fewer parameters
MoE (Mixture of 
Experts)
  Yes
  Sometimes
  No
  Yes
Reduces compute 
per forward pass 
by activating only 
relevant experts
ZeRO 
Optimization 
(DeepSpeed)
 Yes
  Yes
  No
  No
Memory 
optimization for 
large-scale 
training across 
multiple GPUs
FlashAttention
  Yes
  Yes
  No
  Yes
Speeds up 
attention 
computation and 
reduces memory 
overhead
Quantization
  No
  No
  Yes
  Yes
Reduces model 
size and speeds 
up inference by 
using lower-bit 
precision (e.g., 
INT8, FP16)
Knowledge 
Distillation
  Yes
  Yes
  Yes
  Yes
Compresses large 
models into 
smaller, faster 
models by 
transferring 
knowledge
Lab Introduction
 Steps in this Lab
1. Install Required Libraries
1. Uses transformers, bitsandbytes, and accelerate to run quantized models efficiently .
2. Set Up Tokenizer and Model Loading
1. Loads Mistral-7B from Hugging Face.
2. Uses BitsAndBytesConfig to enable quantization.
3. Define an Evaluation Function
1. Runs inference on given prompts.
2. Measures:
1. Inference time
2. Tokens per second
3. VRAM usage
4. Model size
4. Compare 16-bit vs. 4-bit Performance
1. Runs the same set of test prompts on both versions.
2. Plots a performance comparison.
--- End Retrieval ---
--- New Retrieval ---
Optimized Query: Generate an assignment exploring the basic Word2Vec framework using CBOW and SkipGram by coding it using pytorch.
Context:
Word2Vec T raining
‚Ä¢ Word2Vecis apopulartechniqueto learnwordembeddings.It trainson a large
corpusandlearnsto predicteither
‚Äì ContinuousBagof Words:Predictthetargetwordgivenitscontextwords
‚Ä¢ Example:Inthesentence"Thekin giswe ar in ga cr o wn,"theCB OW modelwouldt ak ethe 
contextwordslik eTheandisandtryto predictthetargetwordking
‚Ä¢ Inthiscase ,context is thesurroundingwords,andtheyareal lusedtogetherto mak ea 
prediction
‚Äì Skip-gr am:Predictthecontextwordsgiv enthetargetword
‚Ä¢ Themodeldoesthereverse:itusesthetargetword to predictthecontextwords
‚Ä¢ Example:Inthesentence"Thekin giswe ar in ga cr o wn,"ifthetargetwordisking,themodelwill
tryto predictthesurroundingwordslik eTheandis
‚Ä¢ Ineithermethod,Word2Vecupdatesthewordembeddingsto minimize the
predictionerror
Embedding T echniques
‚Ä¢ Word2Vec
‚Äì CBOW: Context ist hesurroundingwordsusedto predictth etarget
‚Äì Skip-gram:Thet ar getwordisusedto predictth econtext
‚Ä¢ GloVe
‚Äì Capturescontextthroughwordco-occurrenceacrossth eentirecorpus(global
‚Ä¢ context)
‚Ä¢ Self Attention inT ransformerModels:
‚Äì Uses t hese lf-att entionmech an ismto dy nam ic allyfocuson r eleva ntwordsinthe contextwhen
comput ingembeddings
‚Äì BERTan dGPT considerth econtextaroundaword(bidirect ion alforBERT , unidirectionalforGPT)
Word2Vec T raining
CBOW  Model: Context Averaging
‚Ä¢ In theCBOW model,theembeddingsof thecontextwordsareaveraged to formacontextvector
‚Äì The modeltakestheembeddingsofthecontextwords["The","is","wearing"], sumsthemup,anddividesbythe
numberofcontextwords
1
2
3
4
During training, the model adjusts the embeddings so that the 
combined contextvector becomes closer to the actual 
embedding of "king", which is initially [-0.78, 0.56, 0.10].
After training, this combined context vector [-0.09, 0.09, 0.02] will be much closer to the trained embedding of
"king" [-0.75, 0.60, 0.12], allowing the model to correctly predict "king" given the contextwords.
Embedding ‚Äì Next Step after Tokenization
‚ÄúIam a student‚Äù
 Tokenizer Token 
Embedding
I
am
a
student
1
2
3
4
D-dimensional 
Vectors
Embeddings: Starting from Basics
‚ÄúIam a student‚Äù
Tokenizer 
VocabularySize:
ùëá
T okenEmbedding 
EmbeddingDim: 
D
D[0,
182,
874,
19084]
The roleof theembeddingis to 
assigna vectorto eachtoken
D = 2
0,‚ÄúI"
128,‚Äúam"
874,‚Äúa"
19084,‚Äústudent"
ùë•ùúñ‚ÑùùëÅ√óD
BERT Pre-Training
‚Ä¢ BERT (Bidirectional Encoder Representations from Transformers) requires large-scale 
pre-training before fine-tuning for NLP tasks.
‚Ä¢ Pre-training helps the model understand language structures, word relationships, 
and deep contextual meaning.
 Key Points:
 
  Traditional word embeddings (Word2Vec, GloVe) are static, while BERT learns 
contextual embeddings dynamically.
 Unlike GPT (left-to-right) or traditional RNNs, BERT is bidirectional, meaning it 
understands both left and right context simultaneously.
BERT is trained using two self-supervised tasks:
1.Masked Language Model (MLM)
2.Next Sentence Prediction (NSP)
How are Embeddings Created?
There are different generations of word embeddings:
1.Traditional (Static) Embeddings
1.
  Word2Vec (Mikolov, 2013) ‚Äì Predicts words based on context (CBOW & Skip-gram).
2.
  GloVe (Pennington, 2014) ‚Äì Uses word co-occurrence to capture relationships.
3.
  FastText (Bojanowski, 2016) ‚Äì Improves Word2Vec by considering subwords.
2.Modern (Contextual) Embeddings (Used in LLMs)
1.
  BERT, RoBERTa, T5 ‚Äì Generates context-aware embeddings for each word.
2.
  GPT-2, GPT-3 ‚Äì Uses transformer-based embeddings for generation tasks.
3.
  mT5, XLM-R ‚Äì Multilingual embeddings for different languages.
 Key Difference:
‚Ä¢Word2Vec/GloVe ‚Üí Same embedding for "bank" (whether it's "river bank" or "money bank").
‚Ä¢BERT/GPT ‚Üí Different embeddings for "bank" based on context.
BPE: Learning the tokenizer
‚Ä¢ Intuition:startwitheachcharacterasitsown tokenandcombinetokens 
thatco-occuralot
Input: A training corpus (sequence of characters/bytes). 
Initialize the vocabulary V, which is a set of characters. 
While we want to still grow V:
‚Ä¢ Find the pair of elements x,x‚Ä≤‚àà V that co-occur the most
number of times.
‚Ä¢ Replace all occurrences of x,x‚Ä≤ with a new symbol xx‚Ä≤.
‚Ä¢ Add xx‚Ä≤ to V.
BERT Pre-training Tasks
--- End Retrieval ---
--- New Retrieval ---
Optimized Query: make quiz 
Context:
No documents found for provided URLs.
--- End Retrieval ---
--- New Retrieval ---
Optimized Query: make quiz
Context:
No documents found for provided URLs.
--- End Retrieval ---
--- New Retrieval ---
Optimized Query: make quiz on key topics in the lecture such as Lora, mixture of experts etc.
Context:
No documents found for provided URLs.
--- End Retrieval ---
--- New Retrieval ---
Optimized Query: make quiz on key topics in the lecture and generate mcqs true false and short questions
Context:
Additional Reading Materials 
‚Ä¢ ZeRO Optimizer
‚Ä¢ ZeRO-2 & DeepSpeed
‚Ä¢ ZeRO 3
‚Ä¢ Flash Attention
‚Ä¢ Flash Attention 2
‚Ä¢ Knowledge Distillation
Lecture 6: Efficiency in LLMs
Syed Babar Ali School of  Science and Engineering LUMS
Spring 2025
Problem Solving ‚Äì Artificial Intelligence ‚Äì Uncertainty
AI-602: Large Language Models 
Systems
Lab Introduction
 Steps in this Lab
1. Install Required Libraries
1. Uses transformers, bitsandbytes, and accelerate to run quantized models efficiently .
2. Set Up Tokenizer and Model Loading
1. Loads Mistral-7B from Hugging Face.
2. Uses BitsAndBytesConfig to enable quantization.
3. Define an Evaluation Function
1. Runs inference on given prompts.
2. Measures:
1. Inference time
2. Tokens per second
3. VRAM usage
4. Model size
4. Compare 16-bit vs. 4-bit Performance
1. Runs the same set of test prompts on both versions.
2. Plots a performance comparison.
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inferance Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inferance Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inferance Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
How Does Knowledge Distillation Work?
‚Ä¢ A large teacher model is first trained, then its knowledge is transferred to a small student model.
 Step-by-Step Process
1Ô∏è‚É£ Train a large teacher model (e.g., BERT, GPT-4).
2Ô∏è‚É£ Use the teacher‚Äôs outputs (soft labels) to guide the student model instead of raw labels.
3Ô∏è‚É£ Train a small student model to mimic the teacher‚Äôs predictions.
 Key Trick: Soft Labels
‚Ä¢ Instead of using hard labels (correct/incorrect), the student model learns from teacher‚Äôs probability distributions 
(e.g., ‚ÄúParis‚Äù is 90% correct, ‚ÄúLondon‚Äù is 5%, etc.).
‚Ä¢ This helps the student learn subtle patterns and generalize better.
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
--- End Retrieval ---
--- New Retrieval ---
Optimized Query: make quiz on key terms from this lecture and make it hard
Context:
Lecture 4: Tokenization, Embedding and Pre-Training of LLM
Syed Babar Ali School of  Science and Engineering LUMS
Spring 2025
Problem Solving ‚Äì Artificial Intelligence ‚Äì Uncertainty
AI-602: Large Language Models 
Systems
Course 
Journey
‚Ä¢ We looked at a partial history  of language 
models
‚Ä¢ From early N-gr am models through neural network-
based models (including RNNs, LSTMs) to the 
current state-of-the-art T ransformer architecture
‚Ä¢ We also went over how to assess Business 
Requirements and Data Curation necessary for 
Building LLMs
‚Ä¢ Today, let‚Äôs move to Pre-training of LLM and 
related concepts
Steps of LLM training
T okenizer
Training
Self-supervised
Pre-training
Instruction
Finetuning
Reinforcement 
Learningfrom 
Human Feedback
Recognize
Words
T extBook
Reading Doing Exercises Teachers‚Äôfeedback
BERT Pre-training Tasks
Agenda
‚Ä¢ Why Train an LLM from scratch
‚Ä¢ T okenization
‚Ä¢ VectorEmbeddings
‚Ä¢ Transformers
‚Ä¢ Scaling Law
‚Ä¢ Hardware Considerations
‚Ä¢ Pre-Training
Agenda
‚Ä¢ Why Train an LLM from scratch
‚Ä¢ T okenization
‚Ä¢ VectorEmbeddings
‚Ä¢ Transformers
‚Ä¢ Scaling Law
‚Ä¢ Hardware Considerations
‚Ä¢ Pre-Training
BERT Pre-training Tasks: MLM
Agenda
‚Ä¢ Why Train an LLM from scratch
‚Ä¢ T okenization
‚Ä¢ Vector Embeddings
‚Ä¢ Transformers
‚Ä¢ Scaling Law
‚Ä¢ Hardware Considerations
‚Ä¢ Pre-Training
Agenda
‚Ä¢ Why Train an LLM from scratch
‚Ä¢ T okenization
‚Ä¢ Vector Embeddings
‚Ä¢ Transformers
‚Ä¢ Scaling Law
‚Ä¢ Hardware Considerations
‚Ä¢ Pre-Training
Agenda
‚Ä¢ Why Train an LLM from scratch
‚Ä¢ T okenization
‚Ä¢ Vector Embeddings
‚Ä¢ Transformers
‚Ä¢ Scaling Law
‚Ä¢ Hardware Considerations
‚Ä¢ Pre-Training
--- End Retrieval ---
--- New Retrieval ---
Optimized Query: generate quiz on the key terms in this lecture and make it hard
Context:
Lecture 3: Business Problem & Dataset Engineering for LLMs
LUMS Syed Babar Ali School of Science and Engineering 
Spring 2025
Problem Solving ‚Äì Artificial Intelligence ‚Äì Uncertainty
AI-602: Large Language Models 
Systems
Self-Instruct with LLMs
Overview
o Framework for enhancing LLM instruction -
following by generating its own data.
Human Annotations
‚Ä¢ Often times you need to annotate your own data.
o Requires back and forth among annotators to generate the right annotations (Iterative)
o Labor and cost intensive
o Requires strong coordination between scientists, engineers & annotators.
Self-Instruct with LLMs
Overview
o Framework for enhancing LLM instruction -
following by generating its own data.
Process
o Seed Dataset: Start with human -written instructions.
o Generate Instructions: LLM creates new instructions.
o Filter instructions for duplicates, similarity & 
diversity.
o Create Instances: Form input -output pairs; filter for 
quality.
o Fine-tuning: Improve model using the generated 
dataset.
Self-Instruct with LLMs
Overview
o Framework for enhancing LLM instruction -
following by generating its own data.
Process
o Seed Dataset: Start with human -written instructions.
o Generate Instructions: LLM creates new instructions.
o Filter instructions for duplicates, similarity & 
diversity.
o Create Instances: Form input -output pairs; filter for 
quality.
o Fine-tuning: Improve model using the generated 
dataset.
Benefits
o Scalable: Less reliance on human annotation.
o Diverse Tasks: Wider variety improves 
generalization.
o User Alignment: Better understanding of commands.
AI Powered Data Synthesis
‚Ä¢ AI powered data synthesis involves using AI systems instead to speed up 
data generation where:
o System generated data is not available
o Costly to obtain
‚Ä¢ Imagine, you want to train a bot to play chess:
o A game played by humans might take too long.
‚Ä¢ AI based data synthesis also depends on the model training stage.
o Pretraining
‚ñ™ AI crawlers to crawl web, books, papers etc
‚ñ™ Dataset examples include Comopedia, a 25 billion synthetic data set
o Fine tuning
‚ñ™ Instruction fine tuning                Instruction synthesis
‚ñ™ Supervised fine tuning ex. Classification with reasoning                Classification label with reasoning synthesis
Traditional Data Synthesis
Rule-based 
data synthesis
‚Ä¢ Credit Cards
‚Ä¢ Invoices
‚Ä¢ T ax forms
‚Ä¢ Bank Statements
‚Ä¢ T ext to remove gender bias, synonyms etc.
Simulation 
Softwares
‚Ä¢ T est how a self-driving car reacts when encountering a horse on the highway
‚Ä¢ Simulate scenarios with different joint movements and use only the scenarios 
where coffee is successfully poured to train the robot
‚Ä¢ Simulate the Earth‚Äôs systems, climate scientists can create variations in 
temperature changes, precipitation patterns, and extreme weather scenarios.
‚Ä¢ This involves use of established existing systems to generate data for AI 
models.
o Produces coherent data for specific applications
AI Powered Data Synthesis
‚Ä¢ AI powered data synthesis involves using AI systems instead to speed up 
data generation where:
o System generated data is not available
o Costly to obtain
‚Ä¢ Imagine, you want to train a bot to play chess:
o A game played by humans might take too long.
Problem Formulation Frameworks
‚Ä¢ You have a brilliant idea for a new AI 
project. 
o To make it happen, you need to 
convince management to fund your 
idea.
‚Ä¢ AI projects are hard to define
‚Ä¢ Several frameworks to guide problem 
formulation
o AI Canvas
o CRISP-DM
o Design Thinking
Key Takeaways
‚Ä¢ Core Principles of Dataset Design:
o Focus on behaviors you want your model to learn .
o Ensure datasets meet quality, coverage, and quantity criteria.
o High-quality, diverse datasets often outperform larger noisy datasets.
‚Ä¢ Synthetic data:
o A practical solution, especially for finetuning.
o Different training phases (pre -training, instruction finetuning) require tailored datasets.
o Both real and synthetic data must be rigorously evaluated for quality.
o Reliable evaluation methods are essential for adoption.
‚Ä¢ Challenges in Dataset Creation:
o Annotation and verification are harder than automation .
o Creativity and attention to detail are critical in dataset design.
o Overcoming challenges inspires innovative solutions in data synthesis and evaluation.
--- End Retrieval ---
--- New Retrieval ---
Optimized Query: make a quiz focused on the keyterms in the lecture such as Lora, MOE etc.
Context:
Additional Reading Materials 
‚Ä¢ ZeRO Optimizer
‚Ä¢ ZeRO-2 & DeepSpeed
‚Ä¢ ZeRO 3
‚Ä¢ Flash Attention
‚Ä¢ Flash Attention 2
‚Ä¢ Knowledge Distillation
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inferance Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inferance Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inferance Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Lecture 6: Efficiency in LLMs
Syed Babar Ali School of  Science and Engineering LUMS
Spring 2025
Problem Solving ‚Äì Artificial Intelligence ‚Äì Uncertainty
AI-602: Large Language Models 
Systems
Agenda
‚Ä¢ Why Efficiency Matters?  
‚Ä¢ Training Efficiency
‚Ä¢ PEFT  
‚Ä¢ MoE   
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation   
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
Agenda
‚Ä¢ Why Efficiency Matter? 
‚Ä¢ Training Efficiency?
‚Ä¢ PEFT
‚Ä¢ MoE 
‚Ä¢ ZeroOptimization
‚Ä¢ Flash Attention
‚Ä¢ Inference Efficiency
‚Ä¢ Quanitzation 
‚Ä¢ Knowledge Distillation
‚Ä¢ Lab Introduction
--- End Retrieval ---

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "giM74oK1rRIH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79badad7-2833-4836-a2d6-6d2ad643c7c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 357, done.\u001b[K\n",
            "remote: Counting objects: 100% (357/357), done.\u001b[K\n",
            "remote: Compressing objects: 100% (274/274), done.\u001b[K\n",
            "remote: Total 357 (delta 72), reused 307 (delta 68), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (357/357), 9.74 MiB | 20.11 MiB/s, done.\n",
            "Resolving deltas: 100% (72/72), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mevaluation\u001b[0m/  MANIFEST.in     requirements.txt  \u001b[01;34mtests\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mexamples\u001b[0m/    pyproject.toml  \u001b[01;34mscripts\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         LICENSE      README.md       setup.py\n",
            "\u001b[01;34mdocker\u001b[0m/       Makefile     README_zh.md    \u001b[01;34msrc\u001b[0m/\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.51.3)\n",
            "Collecting datasets<=3.5.0,>=2.16.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.6.0)\n",
            "Collecting peft<=0.15.1,>=0.14.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.1)\n",
            "Collecting gradio<=5.25.0,>=4.38.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading gradio-5.25.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.15.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
            "Collecting tiktoken (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n",
            "Collecting uvicorn (from llamafactory==0.9.3.dev0)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting fastapi (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
            "  Downloading sse_starlette-2.3.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
            "Collecting fire (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting omegaconf (from llamafactory==0.9.3.dev0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
            "Collecting numpy<2.0.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<=2.10.6 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
            "Collecting av (from llamafactory==0.9.3.dev0)\n",
            "  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
            "Collecting xxhash (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.15)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
            "Collecting ffmpy (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.17)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.2.1)\n",
            "Collecting pydub (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.3.dev0)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->llamafactory==0.9.3.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->llamafactory==0.9.3.dev0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.20.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.9)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.19.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.25.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.15.1-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.3.3-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire, antlr4-python3-runtime\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26887 sha256=b663f3fdcefbade8ae0870ff9d3d776ccc07857be2e11299b3becfe769b274e9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pz3waxes/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=e761afad62e4c231afd58d2016921c83673a3a6853f181819a39e33f12c01d03\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=138697fdc8fc038e4e9c6627e2b1e784508bc7310dc20cb3048eafe9a7faeeb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built llamafactory fire antlr4-python3-runtime\n",
            "Installing collected packages: pydub, antlr4-python3-runtime, xxhash, uvicorn, tomlkit, shtab, semantic-version, ruff, python-multipart, pydantic-core, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, groovy, fsspec, fire, ffmpy, dill, av, aiofiles, tiktoken, starlette, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, sse-starlette, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio, datasets, bitsandbytes, trl, peft, llamafactory\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.1\n",
            "    Uninstalling pydantic_core-2.33.1:\n",
            "      Successfully uninstalled pydantic_core-2.33.1\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.3\n",
            "    Uninstalling pydantic-2.11.3:\n",
            "      Successfully uninstalled pydantic-2.11.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.15.2\n",
            "    Uninstalling peft-0.15.2:\n",
            "      Successfully uninstalled peft-0.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 antlr4-python3-runtime-4.9.3 av-14.3.0 bitsandbytes-0.45.5 datasets-3.5.0 dill-0.3.8 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 fsspec-2024.12.0 gradio-5.25.0 gradio-client-1.8.0 groovy-0.1.2 llamafactory-0.9.3.dev0 multiprocess-0.70.16 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0 peft-0.15.1 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.8 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.2 sse-starlette-2.3.3 starlette-0.46.2 tiktoken-0.9.0 tomlkit-0.13.2 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.2 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "6cf5496886dd4bcd9c92c7f8b9d10ff2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU environment"
      ],
      "metadata": {
        "id": "H9RXn_YQnn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using our Custom Dataset"
      ],
      "metadata": {
        "id": "TeYs5Lz-QJYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Paths\n",
        "INPUT_PATH  = \"/content/LLaMA-Factory/data/dataset.json\"\n",
        "OUTPUT_PATH = \"/content/LLaMA-Factory/data/dataset_new.json\"\n",
        "\n",
        "# Template with a placeholder for feedback_prompt\n",
        "INSTRUCTION_TEMPLATE = \"\"\"\n",
        "You are reviewing a quiz designed to assess students' theoretical understanding and practical application of topics taught in class.\n",
        "\n",
        "EVALUATE it STRICTLY based on the following criteria. Assign a score out of 10 for each and justify with 1–2 sentences.\n",
        "\n",
        "For each criterion, do the following:\n",
        "1. Give a score out of 10.\n",
        "2. Justify the score with 1–2 sentences.\n",
        "\n",
        "1. **Clarity and Relevance**:\n",
        "  - Are the questions clearly worded and free from ambiguity?\n",
        "  - Are they appropriate for the level of the course and relevant to topics taught?\n",
        "  - Do they reflect the expected knowledge and skill level of students?\n",
        "\n",
        "2. **Coverage of Concepts**:\n",
        "  - Does the quiz cover a diverse and representative set of concepts taught?\n",
        "  - Are both theoretical and practical aspects of the topic included?\n",
        "  - Does it balance breadth and depth appropriately?\n",
        "\n",
        "3. **Question Quality and Structure**:\n",
        "  - Are MCQs structured well with plausible distractors?\n",
        "  - Are True/False statements precise and unambiguous?\n",
        "  - Are short/medium questions open-ended enough to assess understanding, but focused enough to guide students?\n",
        "\n",
        "4. **Cognitive Depth and Usefulness**:\n",
        "  - Do questions vary in difficulty and promote higher-order thinking (not just recall)?\n",
        "  - Are there any case-based or real-world application questions?\n",
        "  - Does it test understanding, analysis, and application?\n",
        "\n",
        "5. **Task Redundancy / Overlap**:\n",
        "  - Tasks should be distinct and may be divided into subtasks if complex.\n",
        "  - Avoid repetition and ensure flow and progression in learning.\n",
        "\n",
        "6. **Feedback Incorporation**:\n",
        "  {feedback_prompt}\n",
        "\n",
        "At the end, write the following in one line exactly:\n",
        "[[[REVIEW_SCHEME]]] = {{ 'clarity': CLARITY_SCORE,\n",
        "                           'coverage': COVERAGE_SCORE,\n",
        "                           'structure': STRUCTURE_SCORE,\n",
        "                           'overlap': OVERLAP_SCORE,\n",
        "                           'depth': DEPTH_SCORE,\n",
        "                           'feedback': FEEDBACK_SCORE }}\n",
        "\"\"\".strip()\n",
        "\n",
        "def wrap_for_alpaca():\n",
        "    with open(INPUT_PATH) as f:\n",
        "        raw = json.load(f)[\"dataset\"]\n",
        "\n",
        "    wrapped = []\n",
        "    for item in raw:\n",
        "        # Fill in the feedback prompt inside the instruction\n",
        "        instruction = INSTRUCTION_TEMPLATE.format(\n",
        "            feedback_prompt=item[\"input\"][\"feedback_prompt\"].strip()\n",
        "        )\n",
        "        # Keep the quiz itself as the input\n",
        "        input_text = item[\"input\"][\"quiz\"].strip()\n",
        "\n",
        "        wrapped.append({\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": input_text,\n",
        "            \"output\": item[\"output\"].strip()\n",
        "        })\n",
        "\n",
        "    with open(OUTPUT_PATH, \"w\") as f:\n",
        "        json.dump(wrapped, f, indent=2)\n",
        "    print(f\"🎉 Written {len(wrapped)} examples to {OUTPUT_PATH}\")\n",
        "\n",
        "\n",
        "wrap_for_alpaca()\n"
      ],
      "metadata": {
        "id": "ap_fvMBsQHJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d14cdf3-9f0b-4d87-e0b8-7497710d52fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 Written 68 examples to /content/LLaMA-Factory/data/dataset_new.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning model via Command Line\n",
        "\n",
        "It takes ~20min for training."
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                                               # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"dataset_new\",                                     # use alpaca and identity datasets\n",
        "  template=\"llama3\",                                         # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                                    # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                                         # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,                             # the micro batch size\n",
        "  gradient_accumulation_steps=4,                             # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                                # use cosine learning rate scheduler\n",
        "  logging_steps=5,                                           # log every 5 steps\n",
        "  warmup_ratio=0.1,                                          # use warmup scheduler\n",
        "  save_steps=1000,                                           # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                                        # the learning rate\n",
        "  num_train_epochs=3.0,                                      # the epochs of training\n",
        "  max_samples=500,                                           # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                                         # clip gradient norm to 1.0\n",
        "  loraplus_lr_ratio=16.0,                                    # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                                                 # use float16 mixed precision training\n",
        "  report_to=\"none\",                                          # disable wandb logging\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "id": "CS0Qk5OR0i4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b4d30a-9378-4bf5-fe60-1511c9f181c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-05-03 11:39:25.843599: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746272366.125504    1507 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746272366.198684    1507 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-03 11:39:26.782831: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|2025-05-03 11:39:39] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "tokenizer_config.json: 100% 51.1k/51.1k [00:00<00:00, 15.3MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 14.3MB/s]\n",
            "special_tokens_map.json: 100% 345/345 [00:00<00:00, 1.91MB/s]\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:41,511 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:41,511 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:41,511 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:41,511 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:41,511 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:41,511 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-05-03 11:39:42,270 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "config.json: 100% 1.26k/1.26k [00:00<00:00, 7.11MB/s]\n",
            "[INFO|configuration_utils.py:693] 2025-05-03 11:39:42,802 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-05-03 11:39:42,812 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:42,914 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:42,914 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:42,915 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:42,915 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:42,915 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:39:42,915 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-05-03 11:39:43,444 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-05-03 11:39:43] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[WARNING|2025-05-03 11:39:43] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
            "[INFO|2025-05-03 11:39:43] llamafactory.data.loader:143 >> Loading dataset dataset_new.json...\n",
            "Generating train split: 68 examples [00:00, 1724.22 examples/s]\n",
            "Converting format of dataset: 100% 68/68 [00:00<00:00, 6371.90 examples/s]\n",
            "Running tokenizer on dataset: 100% 68/68 [00:00<00:00, 297.64 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 2675, 527, 34988, 264, 28223, 6319, 311, 8720, 4236, 6, 32887, 8830, 323, 15325, 3851, 315, 13650, 15972, 304, 538, 13, 19124, 36, 5711, 52, 2390, 433, 30110, 9109, 3196, 389, 279, 2768, 13186, 13, 32739, 264, 5573, 704, 315, 220, 605, 369, 1855, 323, 9541, 449, 220, 16, 4235, 17, 23719, 382, 2520, 1855, 37057, 11, 656, 279, 2768, 512, 16, 13, 21335, 264, 5573, 704, 315, 220, 605, 627, 17, 13, 4702, 1463, 279, 5573, 449, 220, 16, 4235, 17, 23719, 382, 16, 13, 3146, 5176, 10981, 323, 1050, 33194, 334, 512, 220, 482, 8886, 279, 4860, 9539, 3492, 291, 323, 1949, 505, 72868, 5380, 220, 482, 8886, 814, 8475, 369, 279, 2237, 315, 279, 3388, 323, 9959, 311, 13650, 15972, 5380, 220, 482, 3234, 814, 8881, 279, 3685, 6677, 323, 10151, 2237, 315, 4236, 1980, 17, 13, 3146, 67412, 315, 76872, 334, 512, 220, 482, 12838, 279, 28223, 3504, 264, 17226, 323, 18740, 743, 315, 19476, 15972, 5380, 220, 482, 8886, 2225, 32887, 323, 15325, 13878, 315, 279, 8712, 5343, 5380, 220, 482, 12838, 433, 8335, 58321, 323, 8149, 36001, 1980, 18, 13, 3146, 14924, 18410, 323, 29696, 334, 512, 220, 482, 8886, 21539, 48, 82, 34030, 1664, 449, 50434, 8064, 21846, 5380, 220, 482, 8886, 3082, 14, 4139, 12518, 24473, 323, 653, 91313, 5380, 220, 482, 8886, 2875, 14, 27178, 4860, 1825, 84175, 3403, 311, 8720, 8830, 11, 719, 10968, 3403, 311, 8641, 4236, 1980, 19, 13, 3146, 34, 51549, 45020, 323, 5560, 31514, 334, 512, 220, 482, 3234, 4860, 13592, 304, 17250, 323, 12192, 5190, 24747, 7422, 320, 1962, 1120, 19635, 87527, 220, 482, 8886, 1070, 904, 1162, 6108, 477, 1972, 31184, 3851, 4860, 5380, 220, 482, 12838, 433, 1296, 8830, 11, 6492, 11, 323, 3851, 1980, 20, 13, 3146, 6396, 3816, 1263, 6709, 611, 6193, 16338, 334, 512, 220, 482, 47571, 1288, 387, 12742, 323, 1253, 387, 18255, 1139, 1207, 25792, 422, 6485, 627, 220, 482, 35106, 54515, 323, 6106, 6530, 323, 33824, 304, 6975, 382, 21, 13, 3146, 36448, 54804, 367, 334, 512, 220, 1115, 374, 279, 1176, 10165, 779, 3041, 2539, 15785, 320, 605, 14, 605, 696, 1688, 279, 842, 11, 3350, 279, 2768, 304, 832, 1584, 7041, 512, 15873, 58, 793, 21709, 12300, 40532, 5163, 60, 284, 314, 364, 566, 10981, 1232, 7121, 946, 3414, 64827, 345, 6096, 364, 55350, 1232, 7432, 61927, 64827, 345, 6096, 364, 7993, 1232, 60678, 4622, 64827, 345, 6096, 364, 81161, 1232, 36564, 43, 2599, 64827, 345, 6096, 364, 18021, 1232, 3467, 26131, 64827, 345, 6096, 364, 21674, 1232, 29031, 1507, 16218, 64827, 457, 54986, 389, 62924, 4147, 21579, 271, 16, 13, 21539, 48, 25, 3639, 374, 279, 6156, 5915, 315, 60089, 6975, 5380, 32, 8, 33810, 20124, 369, 502, 828, 198, 33, 8, 36480, 828, 1139, 5315, 198, 34, 8, 53253, 828, 13167, 2786, 198, 35, 8, 20400, 502, 828, 198, 34192, 25, 362, 8, 33810, 20124, 369, 502, 828, 13, 27857, 25, 62924, 4147, 6975, 28788, 4211, 389, 30929, 828, 311, 7168, 20124, 369, 64233, 11374, 382, 17, 13, 21539, 48, 25, 16299, 12384, 21877, 4861, 279, 2853, 734, 1701, 20779, 38052, 5380, 32, 8, 735, 5364, 68, 598, 198, 33, 8, 29363, 48570, 198, 34, 8, 6078, 93659, 198, 35, 8, 5345, 3334, 72, 198, 34192, 25, 426, 8, 29363, 48570, 13, 27857, 25, 29363, 31649, 5829, 20779, 38052, 311, 30536, 279, 2853, 734, 369, 1888, 50660, 5137, 382, 18, 13, 21539, 48, 25, 3639, 374, 927, 6410, 1303, 304, 60089, 6975, 5380, 32, 8, 5008, 27772, 1664, 389, 4967, 719, 31555, 389, 1296, 828, 198, 33, 8, 5008, 374, 2288, 4382, 311, 12602, 12912, 198, 34, 8, 5008, 49378, 12248, 304, 828, 198, 35, 8, 5008, 56978, 14268, 389, 682, 828, 198, 34192, 25, 362, 8, 5008, 27772, 1664, 389, 4967, 719, 31555, 389, 1296, 828, 13, 27857, 25, 6193, 6410, 1303, 13980, 994, 264, 1646, 47310, 12248, 304, 4967, 828, 11, 18189, 4689, 2065, 382, 19, 13, 3082, 14, 4139, 25, 362, 5597, 5021, 649, 1193, 3790, 35876, 828, 627, 4139, 13, 27857, 25, 41525, 12690, 649, 3790, 2225, 35876, 323, 70636, 828, 555, 45473, 3196, 389, 4668, 2819, 382, 20, 13, 3082, 14, 4139, 25, 62924, 4147, 6975, 7612, 30929, 828, 627, 2575, 13, 27857, 25, 62924, 4147, 6975, 34744, 389, 1988, 60624, 13840, 311, 5542, 4211, 382, 21, 13, 10928, 22559, 25, 83017, 279, 15837, 86515, 5397, 6696, 1885, 304, 220, 16, 4235, 17, 23719, 627, 16533, 25, 578, 15837, 86515, 5397, 6696, 1885, 39954, 1646, 23965, 26, 1579, 15837, 320, 23796, 4211, 8, 1253, 1234, 6410, 11, 1418, 1579, 33373, 320, 24126, 4211, 8, 1253, 927, 6410, 11, 2225, 28987, 4689, 2065, 382, 22, 13, 10928, 22559, 25, 24702, 7309, 323, 96340, 20779, 38052, 304, 220, 16, 4235, 17, 23719, 627, 16533, 25, 35004, 20779, 38052, 58303, 53249, 927, 279, 4553, 10550, 11, 902, 374, 15528, 719, 6435, 11, 1418, 96340, 20779, 38052, 5829, 832, 6205, 520, 264, 892, 11, 902, 374, 10819, 719, 912, 285, 1291, 13, 128009, 128006, 78191, 128007, 271, 16, 13, 2493, 10981, 323, 1050, 33194, 25, 220, 24, 14, 605, 13, 24271, 527, 9539, 3492, 291, 323, 9959, 311, 60089, 6975, 11, 719, 1063, 21539, 48, 82, 320, 68, 1326, 2637, 1229, 16, 8, 1005, 39532, 78630, 8064, 21846, 430, 1253, 539, 8815, 53243, 40044, 627, 17, 13, 56769, 315, 76872, 25, 220, 24, 14, 605, 13, 578, 28223, 14861, 1401, 13650, 1093, 927, 6410, 1303, 323, 20779, 38052, 11, 719, 37856, 4860, 389, 88394, 12823, 320, 68, 1326, 2637, 445, 16, 7586, 17, 8, 477, 15325, 1646, 16865, 7953, 13708, 627, 18, 13, 16225, 18410, 323, 29696, 25, 220, 23, 14, 605, 13, 21539, 48, 82, 617, 50434, 8064, 21846, 11, 719, 279, 2875, 96877, 4860, 527, 10015, 323, 1436, 22477, 19662, 1139, 15325, 25127, 477, 6696, 33583, 627, 19, 13, 73235, 45020, 323, 5560, 31514, 25, 220, 23, 14, 605, 13, 24271, 1296, 89328, 6677, 11, 719, 2875, 96877, 4860, 320, 68, 1326, 2637, 1229, 21, 11, 1229, 22, 8, 6996, 23965, 11, 33994, 10708, 369, 9200, 7422, 627, 20, 13, 5546, 3816, 1263, 6709, 611, 6193, 16338, 25, 220, 24, 14, 605, 13, 24271, 527, 12742, 11, 719, 1229, 16, 323, 1229, 20, 2225, 5916, 389, 279, 7419, 315, 60089, 6975, 11, 6968, 8275, 44901, 28347, 627, 21, 13, 37957, 54804, 367, 25, 220, 605, 14, 605, 13, 1666, 279, 1176, 10165, 11, 2539, 15785, 527, 12893, 824, 279, 10137, 627, 15873, 58, 793, 21709, 12300, 40532, 5163, 60, 284, 314, 364, 566, 10981, 1232, 220, 24, 11, 364, 55350, 1232, 220, 24, 11, 364, 7993, 1232, 220, 23, 11, 364, 81161, 1232, 220, 24, 11, 364, 18021, 1232, 220, 23, 11, 364, 21674, 1232, 220, 605, 335, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "You are reviewing a quiz designed to assess students' theoretical understanding and practical application of topics taught in class.  \n",
            "\n",
            "EVALUATE it STRICTLY based on the following criteria. Assign a score out of 10 for each and justify with 1–2 sentences.\n",
            "\n",
            "For each criterion, do the following:\n",
            "1. Give a score out of 10.\n",
            "2. Justify the score with 1–2 sentences.\n",
            "\n",
            "1. **Clarity and Relevance**:\n",
            "  - Are the questions clearly worded and free from ambiguity?\n",
            "  - Are they appropriate for the level of the course and relevant to topics taught?\n",
            "  - Do they reflect the expected knowledge and skill level of students?\n",
            "\n",
            "2. **Coverage of Concepts**:\n",
            "  - Does the quiz cover a diverse and representative set of concepts taught?\n",
            "  - Are both theoretical and practical aspects of the topic included?\n",
            "  - Does it balance breadth and depth appropriately?\n",
            "\n",
            "3. **Question Quality and Structure**:\n",
            "  - Are MCQs structured well with plausible distractors?\n",
            "  - Are True/False statements precise and unambiguous?\n",
            "  - Are short/medium questions open-ended enough to assess understanding, but focused enough to guide students?\n",
            "\n",
            "4. **Cognitive Depth and Usefulness**:\n",
            "  - Do questions vary in difficulty and promote higher-order thinking (not just recall)?\n",
            "  - Are there any case-based or real-world application questions?\n",
            "  - Does it test understanding, analysis, and application?\n",
            "\n",
            "5. **Task Redundancy / Overlap**:\n",
            "  - Tasks should be distinct and may be divided into subtasks if complex.\n",
            "  - Avoid repetition and ensure flow and progression in learning.\n",
            "\n",
            "6. **Feedback Incorporation**:\n",
            "  This is the first draft so give full marks (10/10)\n",
            "\n",
            "At the end, write the following in one line exactly:\n",
            "[[[REVIEW_SCHEME]]] = { 'clarity': CLARITY_SCORE,\n",
            "                           'coverage': COVERAGE_SCORE,\n",
            "                          'structure': STRUCTURE_SCORE,\n",
            "                           'overlap': OVERLAP_SCORE,\n",
            "                           'depth': DEPTH_SCORE,\n",
            "                           'feedback': FEEDBACK_SCORE }\n",
            "Quiz on Supervised Learning\n",
            "\n",
            "1. MCQ: What is the primary goal of supervised learning?\n",
            "A) Predict outcomes for new data\n",
            "B) Cluster data into groups\n",
            "C) Reduce data dimensionality\n",
            "D) Generate new data\n",
            "Correct: A) Predict outcomes for new data. Reason: Supervised learning trains models on labeled data to predict outcomes for unseen inputs.\n",
            "\n",
            "2. MCQ: Which algorithm minimizes the cost function using gradient descent?\n",
            "A) K-Means\n",
            "B) Linear Regression\n",
            "C) DBSCAN\n",
            "D) Apriori\n",
            "Correct: B) Linear Regression. Reason: Linear regression uses gradient descent to optimize the cost function for best-fit parameters.\n",
            "\n",
            "3. MCQ: What is overfitting in supervised learning?\n",
            "A) Model performs well on training but poorly on test data\n",
            "B) Model is too simple to capture patterns\n",
            "C) Model ignores noise in data\n",
            "D) Model predicts perfectly on all data\n",
            "Correct: A) Model performs well on training but poorly on test data. Reason: Overfitting occurs when a model learns noise in training data, reducing generalization.\n",
            "\n",
            "4. True/False: A decision tree can only handle numerical data.\n",
            "False. Reason: Decision trees can handle both numerical and categorical data by splitting based on feature values.\n",
            "\n",
            "5. True/False: Supervised learning requires labeled data.\n",
            "True. Reason: Supervised learning relies on input-output pairs to train models.\n",
            "\n",
            "6. Short Answer: Explain the bias-variance tradeoff in 1–2 sentences.\n",
            "Answer: The bias-variance tradeoff balances model complexity; high bias (simple models) may underfit, while high variance (complex models) may overfit, both affecting generalization.\n",
            "\n",
            "7. Short Answer: Compare batch and stochastic gradient descent in 1–2 sentences.\n",
            "Answer: Batch gradient descent computes gradients over the entire dataset, which is stable but slow, while stochastic gradient descent uses one sample at a time, which is faster but noisier.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "1. Clarity and Relevance: 9/10. Questions are clearly worded and relevant to supervised learning, but some MCQs (e.g., Q1) use overly simplistic distractors that may not challenge learners sufficiently.\n",
            "2. Coverage of Concepts: 9/10. The quiz covers key topics like overfitting and gradient descent, but lacks questions on regularization techniques (e.g., L1/L2) or practical model evaluation beyond accuracy.\n",
            "3. Question Quality and Structure: 8/10. MCQs have plausible distractors, but the short-answer questions are brief and could probe deeper into practical implications or tradeoffs.\n",
            "4. Cognitive Depth and Usefulness: 8/10. Questions test foundational knowledge, but short-answer questions (e.g., Q6, Q7) lack complexity, limiting opportunities for critical thinking.\n",
            "5. Task Redundancy / Overlap: 9/10. Questions are distinct, but Q1 and Q5 both touch on the definition of supervised learning, creating slight conceptual overlap.\n",
            "6. Feedback Incorporation: 10/10. As the first draft, full marks are assigned per the prompt.\n",
            "[[[REVIEW_SCHEME]]] = { 'clarity': 9, 'coverage': 9,'structure': 8, 'overlap': 9, 'depth': 8, 'feedback': 10 }<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 13, 2493, 10981, 323, 1050, 33194, 25, 220, 24, 14, 605, 13, 24271, 527, 9539, 3492, 291, 323, 9959, 311, 60089, 6975, 11, 719, 1063, 21539, 48, 82, 320, 68, 1326, 2637, 1229, 16, 8, 1005, 39532, 78630, 8064, 21846, 430, 1253, 539, 8815, 53243, 40044, 627, 17, 13, 56769, 315, 76872, 25, 220, 24, 14, 605, 13, 578, 28223, 14861, 1401, 13650, 1093, 927, 6410, 1303, 323, 20779, 38052, 11, 719, 37856, 4860, 389, 88394, 12823, 320, 68, 1326, 2637, 445, 16, 7586, 17, 8, 477, 15325, 1646, 16865, 7953, 13708, 627, 18, 13, 16225, 18410, 323, 29696, 25, 220, 23, 14, 605, 13, 21539, 48, 82, 617, 50434, 8064, 21846, 11, 719, 279, 2875, 96877, 4860, 527, 10015, 323, 1436, 22477, 19662, 1139, 15325, 25127, 477, 6696, 33583, 627, 19, 13, 73235, 45020, 323, 5560, 31514, 25, 220, 23, 14, 605, 13, 24271, 1296, 89328, 6677, 11, 719, 2875, 96877, 4860, 320, 68, 1326, 2637, 1229, 21, 11, 1229, 22, 8, 6996, 23965, 11, 33994, 10708, 369, 9200, 7422, 627, 20, 13, 5546, 3816, 1263, 6709, 611, 6193, 16338, 25, 220, 24, 14, 605, 13, 24271, 527, 12742, 11, 719, 1229, 16, 323, 1229, 20, 2225, 5916, 389, 279, 7419, 315, 60089, 6975, 11, 6968, 8275, 44901, 28347, 627, 21, 13, 37957, 54804, 367, 25, 220, 605, 14, 605, 13, 1666, 279, 1176, 10165, 11, 2539, 15785, 527, 12893, 824, 279, 10137, 627, 15873, 58, 793, 21709, 12300, 40532, 5163, 60, 284, 314, 364, 566, 10981, 1232, 220, 24, 11, 364, 55350, 1232, 220, 24, 11, 364, 7993, 1232, 220, 23, 11, 364, 81161, 1232, 220, 24, 11, 364, 18021, 1232, 220, 23, 11, 364, 21674, 1232, 220, 605, 335, 128009]\n",
            "labels:\n",
            "1. Clarity and Relevance: 9/10. Questions are clearly worded and relevant to supervised learning, but some MCQs (e.g., Q1) use overly simplistic distractors that may not challenge learners sufficiently.\n",
            "2. Coverage of Concepts: 9/10. The quiz covers key topics like overfitting and gradient descent, but lacks questions on regularization techniques (e.g., L1/L2) or practical model evaluation beyond accuracy.\n",
            "3. Question Quality and Structure: 8/10. MCQs have plausible distractors, but the short-answer questions are brief and could probe deeper into practical implications or tradeoffs.\n",
            "4. Cognitive Depth and Usefulness: 8/10. Questions test foundational knowledge, but short-answer questions (e.g., Q6, Q7) lack complexity, limiting opportunities for critical thinking.\n",
            "5. Task Redundancy / Overlap: 9/10. Questions are distinct, but Q1 and Q5 both touch on the definition of supervised learning, creating slight conceptual overlap.\n",
            "6. Feedback Incorporation: 10/10. As the first draft, full marks are assigned per the prompt.\n",
            "[[[REVIEW_SCHEME]]] = { 'clarity': 9, 'coverage': 9,'structure': 8, 'overlap': 9, 'depth': 8, 'feedback': 10 }<|eot_id|>\n",
            "[INFO|configuration_utils.py:693] 2025-05-03 11:39:44,440 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-05-03 11:39:44,441 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-05-03 11:39:44] llamafactory.model.model_utils.quantization:143 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[INFO|2025-05-03 11:39:44] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|quantization_config.py:436] 2025-05-03 11:39:44,720 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 5.70G/5.70G [00:29<00:00, 195MB/s]\n",
            "[INFO|modeling_utils.py:1124] 2025-05-03 11:40:15,323 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
            "[INFO|modeling_utils.py:2167] 2025-05-03 11:40:15,354 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1142] 2025-05-03 11:40:15,357 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4930] 2025-05-03 11:40:41,760 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4938] 2025-05-03 11:40:41,761 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 220/220 [00:00<00:00, 1.36MB/s]\n",
            "[INFO|configuration_utils.py:1097] 2025-05-03 11:40:41,971 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
            "[INFO|configuration_utils.py:1142] 2025-05-03 11:40:41,971 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-05-03 11:40:42] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-05-03 11:40:42] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-05-03 11:40:42] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-05-03 11:40:42] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-05-03 11:40:42] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,v_proj,up_proj,q_proj,o_proj,down_proj,gate_proj\n",
            "[INFO|2025-05-03 11:40:42] llamafactory.model.loader:143 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "[INFO|trainer.py:748] 2025-05-03 11:40:42,835 >> Using auto half precision backend\n",
            "[INFO|2025-05-03 11:40:43] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2414] 2025-05-03 11:40:43,465 >> ***** Running training *****\n",
            "[INFO|trainer.py:2415] 2025-05-03 11:40:43,465 >>   Num examples = 68\n",
            "[INFO|trainer.py:2416] 2025-05-03 11:40:43,465 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2417] 2025-05-03 11:40:43,465 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2420] 2025-05-03 11:40:43,465 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2421] 2025-05-03 11:40:43,465 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2422] 2025-05-03 11:40:43,465 >>   Total optimization steps = 24\n",
            "[INFO|trainer.py:2423] 2025-05-03 11:40:43,473 >>   Number of trainable parameters = 20,971,520\n",
            "{'loss': 0.9315, 'grad_norm': 0.8507396578788757, 'learning_rate': 4.972077065562821e-05, 'epoch': 0.59}\n",
            "{'loss': 0.5047, 'grad_norm': 0.4256751835346222, 'learning_rate': 4.058724504646834e-05, 'epoch': 1.12}\n",
            "{'loss': 0.265, 'grad_norm': 0.3550877571105957, 'learning_rate': 2.3131747660339394e-05, 'epoch': 1.71}\n",
            "{'loss': 0.2502, 'grad_norm': 0.3014557659626007, 'learning_rate': 6.673703204254347e-06, 'epoch': 2.24}\n",
            "100% 24/24 [11:37<00:00, 29.68s/it][INFO|trainer.py:3984] 2025-05-03 11:52:20,726 >> Saving model checkpoint to llama3_lora/checkpoint-24\n",
            "[INFO|configuration_utils.py:693] 2025-05-03 11:52:21,002 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-05-03 11:52:21,003 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-05-03 11:52:21,191 >> tokenizer config file saved in llama3_lora/checkpoint-24/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-05-03 11:52:21,191 >> Special tokens file saved in llama3_lora/checkpoint-24/special_tokens_map.json\n",
            "[INFO|trainer.py:2681] 2025-05-03 11:52:27,788 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 704.3154, 'train_samples_per_second': 0.29, 'train_steps_per_second': 0.034, 'train_loss': 0.44323497265577316, 'epoch': 2.71}\n",
            "100% 24/24 [11:44<00:00, 29.35s/it]\n",
            "[INFO|trainer.py:3984] 2025-05-03 11:52:27,790 >> Saving model checkpoint to llama3_lora\n",
            "[INFO|configuration_utils.py:693] 2025-05-03 11:52:28,037 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-05-03 11:52:28,038 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-05-03 11:52:28,231 >> tokenizer config file saved in llama3_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-05-03 11:52:28,232 >> Special tokens file saved in llama3_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     2.7059\n",
            "  total_flos               =  8277616GF\n",
            "  train_loss               =     0.4432\n",
            "  train_runtime            = 0:11:44.31\n",
            "  train_samples_per_second =       0.29\n",
            "  train_steps_per_second   =      0.034\n",
            "[INFO|modelcard.py:450] 2025-05-03 11:52:28,478 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",                        # load the saved LoRA adapters\n",
        "  template=\"llama3\",                                         # same to the one in training\n",
        "  finetuning_type=\"lora\",                                    # same to the one in training\n",
        ")\n",
        "chat_model = ChatModel(args)"
      ],
      "metadata": {
        "id": "oh8H9A_25SF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f4f6e3-14f6-46e4-a06c-a599f2645991"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:47,159 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:47,160 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:47,161 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:47,161 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:47,162 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:47,163 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-05-03 11:52:47,641 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:693] 2025-05-03 11:52:48,127 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-05-03 11:52:48,132 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:48,235 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:48,236 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:48,236 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:48,237 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:48,238 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-05-03 11:52:48,241 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-05-03 11:52:48,713 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO|2025-05-03 11:52:48] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[WARNING|2025-05-03 11:52:48] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:693] 2025-05-03 11:52:48,848 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-05-03 11:52:48,850 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO|2025-05-03 11:52:48] llamafactory.model.model_utils.quantization:143 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[INFO|2025-05-03 11:52:48] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|quantization_config.py:436] 2025-05-03 11:52:48,937 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|modeling_utils.py:1124] 2025-05-03 11:52:49,001 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
            "[INFO|modeling_utils.py:2167] 2025-05-03 11:52:49,005 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1142] 2025-05-03 11:52:49,009 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"pad_token_id\": 128255\n",
            "}\n",
            "\n",
            "[INFO|quantizer_bnb_4bit.py:124] 2025-05-03 11:52:49,192 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "[INFO|modeling_utils.py:4930] 2025-05-03 11:53:05,822 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4938] 2025-05-03 11:53:05,824 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1097] 2025-05-03 11:53:05,937 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
            "[INFO|configuration_utils.py:1142] 2025-05-03 11:53:05,939 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO|2025-05-03 11:53:06] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-05-03 11:53:06] llamafactory.model.adapter:143 >> Loaded adapter(s): llama3_lora\n",
            "[INFO|2025-05-03 11:53:06] llamafactory.model.loader:143 >> all params: 8,051,232,768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_chat_model_once(query: str,\n",
        "                         chat_model) -> str:\n",
        "    \"\"\"\n",
        "    Query the LLaMA-Factory chat model once with a given input string and return the response.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input query to send to the model.\n",
        "        model_name_or_path (str): Path or name of the base model.\n",
        "        adapter_name_or_path (str): Path to the LoRA adapters.\n",
        "        template (str): Chat template to use (e.g., 'llama3').\n",
        "        finetuning_type (str): Type of fine-tuning (e.g., 'lora').\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response to the query.\n",
        "    \"\"\"\n",
        "    if not query.strip():\n",
        "        return \"Empty query provided.\"\n",
        "\n",
        "    # Prepare the message\n",
        "    messages = [{\"role\": \"user\", \"content\": query}]\n",
        "\n",
        "    # Generate response using streaming\n",
        "    response = \"\"\n",
        "    for new_text in chat_model.stream_chat(messages):\n",
        "        response += new_text\n",
        "\n",
        "    # Clean up memory\n",
        "    torch_gc()\n",
        "\n",
        "    return response\n",
        "\n",
        "quiz = '''\n",
        "Quiz on Gradient-Based Optimization\\n\\n1. MCQ: Which technique helps escape shallow local minima?\\nA) SGD with momentum\\nB) Batch gradient descent\\nC) L-BFGS\\nD) Early stopping\\nCorrect: A) SGD with momentum. Reason: Momentum carries past gradients.\\n\\n2. True/False: Adam optimizer adapts learning rates per parameter.\\nTrue. Reason: Adam uses estimates of first and second moments.\\n\\n\n",
        "'''\n",
        "feedback_prompt = \"This is the first draft so give full marks only for feedback incorporation (10/10). Other metrics CANNOT have more than 7 marks and need extensive criticism for improvement.\"\n",
        "prompt = f'''\n",
        "  You are a meticulous and brutally honest educator tasked with dissecting and evaluating the quality of a student-designed quiz meant to assess theoretical understanding and practical application of topics taught in class.\n",
        " Your job is not to merely review but to interrogate every choice made in the quiz—question wording, content selection, cognitive depth, and structure—with a fine-toothed comb.\n",
        " Challenge every assumption. Be hyper-critical and assume nothing is adequate unless proven through rigor, clarity, and flawless execution.\n",
        " Expose ambiguity, shallowness, redundancy, poor alignment with learning objectives, and any missed opportunities—no matter how subtle.\n",
        " Even if the quiz seems serviceable, your goal is to highlight weaknesses in coverage, depth, construction, and learning value.\n",
        " Never default to leniency. **Demand perfection, especially in the absence of prior feedback.**\n",
        " The quiz content is below:\n",
        "        =====================\n",
        "        {quiz}\n",
        "        =====================\n",
        "\n",
        "        EVALUATE it STRICTLY based on the following criteria. Assign a score out of 10 for each and justify with detail.\n",
        "\n",
        "        For each criterion, do the following:\n",
        "        1. Give a score out of 10.\n",
        "        2. Justify the score in detail.\n",
        "\n",
        "        1. **Clarity and Relevance**:\n",
        "          - Are the questions clearly worded and free from ambiguity?\n",
        "          - Are they appropriate for the level of the course and relevant to topics taught?\n",
        "          - Do they reflect the expected knowledge and skill level of students?\n",
        "\n",
        "        2. **Coverage of Concepts**:\n",
        "          - Does the quiz cover a diverse and representative set of concepts taught?\n",
        "          - Are both theoretical and practical aspects of the topic included?\n",
        "          - Does it balance breadth and depth appropriately?\n",
        "\n",
        "        3. **Question Quality and Structure**:\n",
        "          - Does the quiz contain the required 3-5 MCQs, 2-3 True/False statements, and 4-5 short/medium-length questions?\n",
        "          - Are MCQs structured well with plausible distractors?\n",
        "          - Are True/False statements precise and unambiguous?\n",
        "          - Are short/medium questions open-ended enough to assess understanding, but focused enough to guide students?\n",
        "\n",
        "        4. **Cognitive Depth and Usefulness**:\n",
        "          - Do questions vary in difficulty and promote higher-order thinking (not just recall)?\n",
        "          - Are there any case-based or real-world application questions?\n",
        "          - Does it test understanding, analysis, and application?\n",
        "\n",
        "        5. **Task Redundancy / Overlap**:\n",
        "          - Tasks should be distinct and may be divided into subtasks if complex.\n",
        "          - Avoid repetition and ensure flow and progression in learning.\n",
        "\n",
        "        6. **Feedback Incorporation**:\n",
        "          - {feedback_prompt}\n",
        "          - If this is the first draft (i.e. no previous feedback), be EXTREMELY CRITICAL in other fields. ONLY give scores above 7 for criteria that are exceptionally well-executed with zero flaws. Assume perfection is expected on first draft to drive improvement.\n",
        "\n",
        "         At the end, write the following in one line ... [[[REVIEW_SCHEME]]] = {{ 'clarity': CLARITY_SCORE, 'coverage': COVERAGE_SCORE, 'structure': STRUCTURE_SCORE, 'overlap': OVERLAP_SCORE, 'depth': DEPTH_SCORE, 'feedback': FEEDBACK_SCORE }}\n",
        "'''\n",
        "query = prompt\n",
        "response = query_chat_model_once(query,chat_model)\n",
        "print(f\"User: {query}\")\n",
        "print(f\"Assistant: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8hgyP9W84CI",
        "outputId": "4f8105d6-268c-4b5b-cb77-be6b327ec62d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: \n",
            "  You are a meticulous and brutally honest educator tasked with dissecting and evaluating the quality of a student-designed quiz meant to assess theoretical understanding and practical application of topics taught in class.\n",
            " Your job is not to merely review but to interrogate every choice made in the quiz—question wording, content selection, cognitive depth, and structure—with a fine-toothed comb.\n",
            " Challenge every assumption. Be hyper-critical and assume nothing is adequate unless proven through rigor, clarity, and flawless execution.\n",
            " Expose ambiguity, shallowness, redundancy, poor alignment with learning objectives, and any missed opportunities—no matter how subtle.\n",
            " Even if the quiz seems serviceable, your goal is to highlight weaknesses in coverage, depth, construction, and learning value.\n",
            " Never default to leniency. **Demand perfection, especially in the absence of prior feedback.**\n",
            " The quiz content is below:\n",
            "        =====================\n",
            "        \n",
            "Quiz on Gradient-Based Optimization\n",
            "\n",
            "1. MCQ: Which technique helps escape shallow local minima?\n",
            "A) SGD with momentum\n",
            "B) Batch gradient descent\n",
            "C) L-BFGS\n",
            "D) Early stopping\n",
            "Correct: A) SGD with momentum. Reason: Momentum carries past gradients.\n",
            "\n",
            "2. True/False: Adam optimizer adapts learning rates per parameter.\n",
            "True. Reason: Adam uses estimates of first and second moments.\n",
            "\n",
            "\n",
            "\n",
            "        =====================\n",
            "\n",
            "        EVALUATE it STRICTLY based on the following criteria. Assign a score out of 10 for each and justify with detail.\n",
            "        \n",
            "        For each criterion, do the following:\n",
            "        1. Give a score out of 10.\n",
            "        2. Justify the score in detail.\n",
            "\n",
            "        1. **Clarity and Relevance**:\n",
            "          - Are the questions clearly worded and free from ambiguity?\n",
            "          - Are they appropriate for the level of the course and relevant to topics taught?\n",
            "          - Do they reflect the expected knowledge and skill level of students?\n",
            "\n",
            "        2. **Coverage of Concepts**:\n",
            "          - Does the quiz cover a diverse and representative set of concepts taught?\n",
            "          - Are both theoretical and practical aspects of the topic included?\n",
            "          - Does it balance breadth and depth appropriately?\n",
            "\n",
            "        3. **Question Quality and Structure**:\n",
            "          - Does the quiz contain the required 3-5 MCQs, 2-3 True/False statements, and 4-5 short/medium-length questions?\n",
            "          - Are MCQs structured well with plausible distractors?\n",
            "          - Are True/False statements precise and unambiguous?\n",
            "          - Are short/medium questions open-ended enough to assess understanding, but focused enough to guide students?\n",
            "\n",
            "        4. **Cognitive Depth and Usefulness**:\n",
            "          - Do questions vary in difficulty and promote higher-order thinking (not just recall)?\n",
            "          - Are there any case-based or real-world application questions?\n",
            "          - Does it test understanding, analysis, and application?\n",
            "\n",
            "        5. **Task Redundancy / Overlap**:\n",
            "          - Tasks should be distinct and may be divided into subtasks if complex.\n",
            "          - Avoid repetition and ensure flow and progression in learning.\n",
            "\n",
            "        6. **Feedback Incorporation**:\n",
            "          - This is the first draft so give full marks only for feedback incorporation (10/10). Other metrics CANNOT have more than 7 marks and need extensive criticism for improvement.\n",
            "          - If this is the first draft (i.e. no previous feedback), be EXTREMELY CRITICAL in other fields. ONLY give scores above 7 for criteria that are exceptionally well-executed with zero flaws. Assume perfection is expected on first draft to drive improvement.\n",
            "\n",
            "         At the end, write the following in one line ... [[[REVIEW_SCHEME]]] = { 'clarity': CLARITY_SCORE, 'coverage': COVERAGE_SCORE, 'structure': STRUCTURE_SCORE, 'overlap': OVERLAP_SCORE, 'depth': DEPTH_SCORE, 'feedback': FEEDBACK_SCORE }\n",
            "\n",
            "Assistant: 1. Clarity and Relevance: 9/10. Questions are clear, but the distractor in Q1 is too simplistic.\n",
            "2. Coverage of Concepts: 8/10. Covers optimization techniques but lacks scenario-based questions.\n",
            "3. Question Quality and Structure: 8/10. The distractor in Q1 is too basic and the TF question lacks context.\n",
            "4. Cognitive Depth and Usefulness: 7/10. Mostly recall-based with limited application.\n",
            "5. Task Redundancy / Overlap: 9/10. Questions are distinct but could be more varied.\n",
            "6. Feedback Incorporation: 10/10. As the first draft, full marks are assigned.\n",
            "[[[REVIEW_SCHEME]]] = { 'clarity':9,'coverage':8,'structure':8,'overlap':9,'depth':7,'feedback':10 }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following this the adapter weights and configs were downloaded from ```llama3_lora folder``` and uploaded to [HuggingFace Models](https://huggingface.co/mahmad1882/llama3-8b-instruct-verification-lora). From there we use Together AI's serverless inference API supporting custom adapters in free tier for inference in our verification step of quiz generation."
      ],
      "metadata": {
        "id": "CcbgN_w_8Pub"
      }
    }
  ]
}
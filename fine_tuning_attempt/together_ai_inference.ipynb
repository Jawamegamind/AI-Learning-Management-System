{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90386b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Response: {'data': {'job_id': 'job-aaf23828-0950-4931-92cb-10fedc618c3d', 'model_name': 'muhammadahmad1/test-lora-model-creation-8b', 'model_id': 'endpoint-dc405e1c-9ee8-464a-9436-972e0c59d0c0', 'model_source': 'huggingface'}, 'message': 'Processing model weights. Job created.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Define the values (you can hardcode or use os.environ for secrets)\n",
    "TOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")  # or paste your Together API key here\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")  # or paste your Hugging Face token here\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model_name\": \"test-lora-model-creation-8b\",\n",
    "    \"model_source\": \"https://huggingface.co/mahmad1882/llama3-8b-instruct-verification-lora\",\n",
    "    \"model_type\": \"adapter\",\n",
    "    \"description\": \"test_lora_8B\",\n",
    "    \"hf_token\": HF_TOKEN\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.together.xyz/v0/models\", headers=headers, json=data)\n",
    "\n",
    "print(\"Status:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e5b2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"adapter_upload\",\n",
      "  \"job_id\": \"job-aaf23828-0950-4931-92cb-10fedc618c3d\",\n",
      "  \"status\": \"Complete\",\n",
      "  \"status_updates\": [\n",
      "    {\n",
      "      \"status\": \"Queued\",\n",
      "      \"message\": \"Job has been created\",\n",
      "      \"timestamp\": \"2025-05-03T12:41:58Z\"\n",
      "    },\n",
      "    {\n",
      "      \"status\": \"Running\",\n",
      "      \"message\": \"Received job from queue, starting\",\n",
      "      \"timestamp\": \"2025-05-03T12:42:21Z\"\n",
      "    },\n",
      "    {\n",
      "      \"status\": \"Running\",\n",
      "      \"message\": \"Adapter download in progress\",\n",
      "      \"timestamp\": \"2025-05-03T12:42:21Z\"\n",
      "    },\n",
      "    {\n",
      "      \"status\": \"Running\",\n",
      "      \"message\": \"Adapter bf16 conversion in progress\",\n",
      "      \"timestamp\": \"2025-05-03T12:42:24Z\"\n",
      "    },\n",
      "    {\n",
      "      \"status\": \"Running\",\n",
      "      \"message\": \"Adapter validation in progress\",\n",
      "      \"timestamp\": \"2025-05-03T12:42:24Z\"\n",
      "    },\n",
      "    {\n",
      "      \"status\": \"Running\",\n",
      "      \"message\": \"Adapter upload in progress\",\n",
      "      \"timestamp\": \"2025-05-03T12:42:24Z\"\n",
      "    },\n",
      "    {\n",
      "      \"status\": \"Running\",\n",
      "      \"message\": \"Model entity update in progress\",\n",
      "      \"timestamp\": \"2025-05-03T12:42:34Z\"\n",
      "    },\n",
      "    {\n",
      "      \"status\": \"Running\",\n",
      "      \"message\": \"Warming up adapter\",\n",
      "      \"timestamp\": \"2025-05-03T12:42:35Z\"\n",
      "    },\n",
      "    {\n",
      "      \"status\": \"Running\",\n",
      "      \"message\": \"Adapter loaded and warmed\",\n",
      "      \"timestamp\": \"2025-05-03T12:42:44Z\"\n",
      "    },\n",
      "    {\n",
      "      \"status\": \"Complete\",\n",
      "      \"message\": \"Job is Complete\",\n",
      "      \"timestamp\": \"2025-05-03T12:42:44Z\"\n",
      "    }\n",
      "  ],\n",
      "  \"args\": {\n",
      "    \"description\": \"test_lora_8B\",\n",
      "    \"modelName\": \"muhammadahmad1/test-lora-model-creation-8b\",\n",
      "    \"modelSource\": \"mahmad1882/llama3-8b-instruct-verification-lora\"\n",
      "  },\n",
      "  \"created_at\": \"2025-05-03T12:41:58Z\",\n",
      "  \"updated_at\": \"2025-05-03T12:42:44Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "job_id = 'job-aaf23828-0950-4931-92cb-10fedc618c3d'\n",
    "\n",
    "# Setup headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\"\n",
    "}\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(f\"https://api.together.xyz/v1/jobs/{job_id}\", headers=headers)\n",
    "\n",
    "# Pretty print the JSON response\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9ac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Model Response:\n",
      "```\n",
      "1. Clarity and Relevance: 8/10. Questions are clear but one MCQ on gradient descent is too simplistic.\n",
      "2. Coverage of Concepts: 9/10. Comprehensive but lacks questions on evaluation metrics or detailed applications.\n",
      "3. Question Quality and Structure: 9/10. Structured well with one poorly worded MCQ on gradient descent.\n",
      "4. Cognitive Depth and Usefulness: 8/10. Does not challenge deeply with overly basic new question.\n",
      "5. Task Redundancy / Overlap: 9/10. Distinct but one new question is too shallow.\n",
      "6. Feedback Incorporation: 5/10. Failed to include new question types despite detailed instructions.\n",
      "[[[REVIEW_SCHEME]]] = { 'clarity':8,'coverage':9,'structure':9,'overlap':9,'depth':8,'feedback':5 } = 7.4/10. Reviewer is disappointed with lack of critical engagement with detailed feedback, particularly regarding question type and depth. Reviewer scored high for clarity and coverage due to the inclusion of application and theory questions but emphasized the need for deeper cognitive engagement. Overlap was minimal, but the poorly constructed new question significantly reduced the overall score. To improve, the quiz should focus on real-world applications and challenge students with open-ended, scenario-based questions that require critical thinking, much like the application question on healthcare. Reviewer concludes with a score of 7.4/10, reflecting both the quiz's positive elements and its failure to incorporate new challenge types as instructed.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "MODEL_NAME_FOR_INFERENCE = \"muhammadahmad1/test-lora-model-creation-8b\"\n",
    "\n",
    "# Step 1: Submit the inference job\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "quiz = \"Quiz on Supervised Learning\\n\\n1. MCQ: What is the primary goal of supervised learning?\\nA) Predict outcomes for new data\\nB) Cluster data into groups\\nC) Reduce data dimensionality\\nD) Generate new data\\nCorrect: A) Predict outcomes for new data. Reason: Supervised learning trains models on labeled data to predict outcomes for unseen inputs.\\n\\n2. MCQ: Which algorithm minimizes the cost function using gradient descent?\\nA) K-Means\\nB) Linear Regression\\nC) DBSCAN\\nD) Apriori\\nCorrect: B) Linear Regression. Reason: Linear regression uses gradient descent to optimize the cost function for best-fit parameters.\\n\\n3. MCQ: What is overfitting in supervised learning?\\nA) Model performs well on training but poorly on test data\\nB) Model is too simple to capture patterns\\nC) Model ignores noise in data\\nD) Model predicts perfectly on all data\\nCorrect: A) Model performs well on training but poorly on test data. Reason: Overfitting occurs when a model learns noise in training data, reducing generalization.\\n\\n4. True/False: A decision tree can only handle numerical data.\\nFalse. Reason: Decision trees can handle both numerical and categorical data by splitting based on feature values.\\n\\n5. True/False: Supervised learning requires labeled data.\\nTrue. Reason: Supervised learning relies on input-output pairs to train models.\\n\\n6. Short Answer: Explain the bias-variance tradeoff in 1‚Äì2 sentences.\\nAnswer: The bias-variance tradeoff balances model complexity; high bias (simple models) may underfit, while high variance (complex models) may overfit, both affecting generalization.\\n\\n7. Short Answer: Compare batch and stochastic gradient descent in 1‚Äì2 sentences.\\nAnswer: Batch gradient descent computes gradients over the entire dataset, which is stable but slow, while stochastic gradient descent uses one sample at a time, which is faster but noisier.\\n\\n8. MCQ: Which metric is used to evaluate classification models?\\nA) Mean Squared Error\\nB) Accuracy\\nC) R-squared\\nD) Mean Absolute Error\\nCorrect: B) Accuracy. Reason: Accuracy measures the proportion of correct predictions in classification.\\n\\n9. Short Answer: Describe a real-world application of supervised learning in healthcare.\\nAnswer: Supervised learning can be used to predict disease outcomes based on patient data, such as classifying whether a patient has a particular condition.\\n\\n10. Short Answer: Explain why precision and recall are important in imbalanced classification problems.\\nAnswer: In imbalanced datasets, accuracy can be misleading; precision and recall provide better insights into model performance on minority classes.\"\n",
    "\n",
    "feedback_prompt = \"Feedback: Include questions on evaluation metrics and real-world applications. If the given feedback has NOT been FULLY incorporated, PENALIZE HARSHLY.\"\n",
    "prompt = f'''\n",
    " You are a meticulous and brutally honest educator tasked with dissecting and evaluating the quality of a student-designed quiz meant to assess theoretical understanding and practical application of topics taught in class.\n",
    " Your job is not to merely review but to interrogate every choice made in the quiz‚Äîquestion wording, content selection, cognitive depth, and structure‚Äîwith a fine-toothed comb.\n",
    " Challenge every assumption. Be hyper-critical and assume nothing is adequate unless proven through rigor, clarity, and flawless execution.\n",
    " Expose ambiguity, shallowness, redundancy, poor alignment with learning objectives, and any missed opportunities‚Äîno matter how subtle.\n",
    " Even if the quiz seems serviceable, your goal is to highlight weaknesses in coverage, depth, construction, and learning value.\n",
    " Never default to leniency. **Demand perfection, especially in the absence of prior feedback.**\n",
    " The quiz content is below:\n",
    "        =====================\n",
    "        {quiz}\n",
    "        =====================\n",
    "\n",
    "        EVALUATE it STRICTLY based on the following criteria. Assign a score out of 10 for each and justify with detail.\n",
    "        \n",
    "        For each criterion, do the following:\n",
    "        1. Give a score out of 10.\n",
    "        2. Justify the score in detail.\n",
    "\n",
    "        1. **Clarity and Relevance**:\n",
    "          - Are the questions clearly worded and free from ambiguity?\n",
    "          - Are they appropriate for the level of the course and relevant to topics taught?\n",
    "          - Do they reflect the expected knowledge and skill level of students?\n",
    "\n",
    "        2. **Coverage of Concepts**:\n",
    "          - Does the quiz cover a diverse and representative set of concepts taught?\n",
    "          - Are both theoretical and practical aspects of the topic included?\n",
    "          - Does it balance breadth and depth appropriately?\n",
    "\n",
    "        3. **Question Quality and Structure**:\n",
    "          - Does the quiz contain the required 3-5 MCQs, 2-3 True/False statements, and 4-5 short/medium-length questions?\n",
    "          - Are MCQs structured well with plausible distractors?\n",
    "          - Are True/False statements precise and unambiguous?\n",
    "          - Are short/medium questions open-ended enough to assess understanding, but focused enough to guide students?\n",
    "\n",
    "        4. **Cognitive Depth and Usefulness**:\n",
    "          - Do questions vary in difficulty and promote higher-order thinking (not just recall)?\n",
    "          - Are there any case-based or real-world application questions?\n",
    "          - Does it test understanding, analysis, and application?\n",
    "\n",
    "        5. **Task Redundancy / Overlap**:\n",
    "          - Tasks should be distinct and may be divided into subtasks if complex.\n",
    "          - Avoid repetition and ensure flow and progression in learning.\n",
    "\n",
    "        6. **Feedback Incorporation**:\n",
    "          - {feedback_prompt}\n",
    "          - If this is the first draft (i.e. no previous feedback), be EXTREMELY CRITICAL in other fields. ONLY give scores above 7 for criteria that are exceptionally well-executed with zero flaws. Assume perfection is expected on first draft to drive improvement.\n",
    "\n",
    "         At the end, write the following in one line ... [[[REVIEW_SCHEME]]] = {{ 'clarity': CLARITY_SCORE, 'coverage': COVERAGE_SCORE, 'structure': STRUCTURE_SCORE, 'overlap': OVERLAP_SCORE, 'depth': DEPTH_SCORE, 'feedback': FEEDBACK_SCORE }}\n",
    "'''\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL_NAME_FOR_INFERENCE,\n",
    "    \"prompt\": prompt,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.together.xyz/v1/completions\", headers=headers, json=payload)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Error submitting job: {response.status_code}, {response.text}\")\n",
    "\n",
    "completion_output = response.json().get(\"choices\", [{}])[0].get(\"text\", \"\").strip()\n",
    "if not completion_output:\n",
    "    raise Exception(\"No output returned by the model.\")\n",
    "\n",
    "if completion_output:\n",
    "    print(\"\\nüß† Model Response:\")\n",
    "    print(completion_output)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No output returned by the model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
